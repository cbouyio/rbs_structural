{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet RBS structural "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "...\n",
    "RBPs : qlqs lignes + figure + role dans régulation + role spécifique dans Traduction \n",
    "\n",
    "Les reads pour les protéines de liaison à l'ARN (RBP) reflètent le nombre d'interactions entre une RBP spécifique et un ARN particulier associé à chaque gène dans un tableau. Ces données sont généralement obtenues par des techniques comme eCLIP (Enhanced CrossLinking and ImmunoPrecipitation), qui permettent de détecter les sites de liaison en utilisant un cross-linking chimique pour stabiliser l'interaction entre les RBPs et l'ARN, suivi d'une immunoprécipitation pour isoler les complexes RBP-ARN. (generalement dans introduction). \n",
    "\n",
    "provenance données : Sypher \n",
    "- expliquer exactement comment Sypher a recuperé ces données\n",
    "\n",
    "Pblm : Trouver les motifs structurales des RBPs qui sont impliqués dans les liaisons à l'ARNm concernant spécifiquement la traduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data \n",
    "\n",
    "## Input Data \n",
    "\n",
    "- Pre-processing of the data : \n",
    "how he obtained the data \n",
    "find a exp ribosome profiling, trouver des cells lines that are same \n",
    "data from E-Clip exp -> extended from a data ribosome profile \n",
    "RBP df \n",
    "\n",
    "Fichier : data_raw.csv \n",
    "Le fichier contient  17,670 lignes, ou chaque ligne représente un gène spécifique, et il y a 144 colonnes. \n",
    "La première colonne, Geneid, contient les identifiants des gènes.\n",
    "Les colonnes suivantes représentent des valeurs numériques associées aux RBPs pour chaque gène, données proveant de e-CLIP.\n",
    "Les quatre dernières colonnes contiennent les données de ribosome profiling d'une meme lignée cellulaire depuis RPF db. \n",
    "\n",
    "Une fois isolés, les complexes sont séquencés pour identifier les fragments d'ARN liés à chaque RBP, ce qui permet de quantifier le nombre de reads pour chaque gène. Ces chiffres sont essentiels car ils fournissent des informations sur le rôle des RBPs dans la régulation post-transcriptionnelle.\n",
    "\n",
    "4 dernières colonnes = conditions expérimentales spécifiques :\n",
    "\n",
    "- shScramble : Données de contrôle obtenues avec un siRNA \"scramble\" qui ne cible aucun gène, servant de référence pour évaluer les autres traitements.\n",
    "- shDDX41 : Données où l'expression du gène DDX41 est inhibée par un shRNA spécifique. Cela permet d'étudier l'impact de cette inhibition sur la traduction des gènes, DDX41 étant impliqué dans la régulation de la traduction et de la maturation des ARN.\n",
    "- DMSO : Données pour les cellules traitées avec DMSO, un solvant utilisé comme contrôle pour s'assurer que les effets observés ne proviennent pas simplement de la présence du solvant.\n",
    "- CX5461 : Données pour les cellules traitées avec CX5461, un composé inhibant la synthèse des ARN ribosomiques. Cela permet d'analyser l'effet de ce traitement sur la traduction des gènes en comparaison avec les conditions de contrôle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/data_raw.csv')\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation données \n",
    "Vu que les 4 dernieres colonnes correspondent à des conditions experimentales (pas des RBPs directement), on va séparer le data_raw en 2 jeu de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/data_raw.csv\")\n",
    "\n",
    "df_eclip = df.iloc[:, :-4]\n",
    "df_rpf = df.iloc[:, [0] + list(range(len(df.columns)-4, len(df.columns)))]  # Garde la première colonne + les 4 dernières\n",
    "\n",
    "df_eclip.to_csv(\"df_eclip.csv\", index=False, header=True)\n",
    "df_rpf.to_csv(\"df_rpf.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche des motifs structuraux des RBPs du data \n",
    "\n",
    "## 1- Récupération des Identifiants UniProt\n",
    "Ecriture d'un script Python qui utilise l'API UniProt pour rechercher les identifiants des protéines RBP en fonction de leurs noms, contenu dans le fichier df_eclip.csv. La fonction get_uniprot_ids a été conçue pour traiter chaque nom de RBP et récupérer son identifiant UniProt correspondant, en veillant à ce que seuls les identifiants associés aux protéines humaines soient retenus. \n",
    "A l'issue de cette étape, on recupere les identifants de nos 139 RBPs. \n",
    "\n",
    "## 2- Création des URLs d'UniProt\n",
    "Pour faciliter l'accès aux pages détaillées des RBPs sur le site d'UniProt, nous avons généré des URLs à partir des identifiants obtenus. Ces URLs ont été enregistrées dans un fichier CSV (uniprot_urls.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execution en 1m36s\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "def get_uniprot_ids(rbp_names):\n",
    "    \"\"\"\n",
    "    Récupère les identifiants UniProt pour une liste de noms de protéines (RBPs).\n",
    "\n",
    "    Args:\n",
    "        rbp_names (list): Une liste de noms de protéines (RBPs) à rechercher.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un dictionnaire contenant les identifiants UniProt pour chaque RBP,\n",
    "              ou un message d'erreur si aucun identifiant n'est trouvé.\n",
    "    \"\"\"\n",
    "    # URL de l'API UniProt pour la recherche\n",
    "    url = \"https://rest.uniprot.org/uniprotkb/search\"\n",
    "    \n",
    "    # Dictionnaire (noms RBPs comme key et uniprot_id comme value)\n",
    "    uniprot_ids = {} \n",
    "\n",
    "    for rbp in rbp_names:\n",
    "        params = {\n",
    "            \"query\": rbp,  # Recherche par nom de RBP\n",
    "            \"format\": \"json\",   # Format JSON\n",
    "            \"size\": 10          # Limite le nombre de résultats à 10\n",
    "        }\n",
    "        \n",
    "        # Envoi de la requête GET à l'API\n",
    "        response = requests.get(url, params=params)\n",
    "        \n",
    "        # Vérification du statut de la requête\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Filtre les résultats pour trouver la protéine humaine\n",
    "            for result in data.get('results', []):\n",
    "                if 'HUMAN' in result.get('uniProtkbId', ''):\n",
    "                    uniprot_ids[rbp] = result['primaryAccession']\n",
    "                    break  \n",
    "            else:\n",
    "                uniprot_ids[rbp] = \"Aucun identifiant trouvé pour cette protéine.\"\n",
    "        else:\n",
    "            uniprot_ids[rbp] = f\"Erreur: {response.status_code}\"\n",
    "    \n",
    "    return uniprot_ids\n",
    "\n",
    "\n",
    "df = pd.read_csv('df_eclip.csv')  \n",
    "# Extraire les noms des RBPs à partir de la première ligne en excluant la colonne 'geneid'\n",
    "rbp_names = df.columns.tolist()[1:] \n",
    "\n",
    "uniprot_results = get_uniprot_ids(rbp_names)\n",
    "print(f\"{'Nom de la protéine':<20} {'Identifiant UniProt'}\")\n",
    "print('-' * 40)\n",
    "for rbp, uniprot_id in uniprot_results.items():\n",
    "    print(f\"{rbp:<20} {uniprot_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "base_url = \"https://www.uniprot.org/uniprot/\"\n",
    "uniprot_urls = {}\n",
    "\n",
    "# Enregistrement des URLs dans un fichier CSV\n",
    "with open(\"uniprot_urls.csv\", \"w\", newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Nom de la protéine', 'URL UniProt'])  # Header\n",
    "    for rbp, uniprot_id in uniprot_results.items():\n",
    "        if \"Aucun\" not in uniprot_id and \"Erreur\" not in uniprot_id:\n",
    "            url = f\"{base_url}{uniprot_id}\"\n",
    "            uniprot_urls[rbp] = url\n",
    "            csv_writer.writerow([rbp, url])\n",
    "        else:\n",
    "            csv_writer.writerow([rbp, uniprot_id])\n",
    "\n",
    "print(\"\\nLes URLs ont été enregistrées dans 'uniprot_urls.csv'.\")\n",
    "\n",
    "# Checking que le fichier est bon \n",
    "import pandas as pd\n",
    "df_urls = pd.read_csv('uniprot_urls.csv')\n",
    "print(f\"Nombre total d'URLs dans le fichier: {len(df_urls)}\")\n",
    "print(\"\\nLes 5 premières lignes du fichier sont :\")\n",
    "print(df_urls.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Recupération des informations structurales \n",
    "\n",
    "L'API UniProt permet de récupérer des informations détaillées sur les protéines, notamment les features qui décrivent les régions fonctionnelles et structurelles. Pour chaque identifiant UniProt, nous avons effectué des requêtes API pour extraire des données au format JSON. Les features comprennent des informations telles que le type, la description, la localisation, l'identifiant et les preuves associées. \n",
    "Ces données ont été organisées et stockées dans un fichier CSV (uniprot_structural_info.csv), facilitant ainsi l'analyse et la visualisation des caractéristiques structurales des protéines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Tri des informations et récupération des identifiants des BDD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06/11 update : \n",
    "Nous avons repris le fichier précédent pour nettoyer les données et uniquement gardés celles qu'on sera amener à utiliser pour la suite du projet. \n",
    "\n",
    "Dans le fichier final on garde les informations structurales essentielles pour chaque protéine RBP. Il inclut une colonne No_feat, qui indique le nombre total de caractéristiques structurales identifiées pour chaque RBP. Les informations clés sur les motifs et les sites de liaison sont regroupées dans une liste, motif_list, qui résume uniquement les caractéristiques pertinentes de type \"motif\" et \"binding site\".\n",
    "Trois autres colonnes, Binding_site, Motifs, et Domains, présentent les identifiants des evidences (bases de données de provenance) à chaque type de caractéristique, séparés par des points-virgules. Chaque identifiant n'apparaît qu'une seule fois pour éviter les doublons. Pour les sites de liaison, l’identifiant est complété par un code d’évidence lorsque nécessaire. \n",
    "En fin de compte, le fichier csv fournit un résumé des informations par protéine, avec des colonnes claires pour le nom de la protéine (rbp_name), son identifiant UniProt (uniprotID), le nombre total de caractéristiques (No_feat), et les identifiants des motifs, sites de liaison, et domaines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_structural_info(uniprot_results):\n",
    "    \"\"\"\n",
    "    Récupère les informations structurelles pour une liste d'identifiants UniProt, incluant les motifs, domaines et sites de liaison.\n",
    "\n",
    "    Args:\n",
    "        uniprot_results (dict): Un dictionnaire contenant des noms de RBPs et leurs identifiants UniProt.\n",
    "\n",
    "    Returns:\n",
    "        list: Une liste de dictionnaires contenant les informations structurelles pour chaque identifiant.\n",
    "    \"\"\"\n",
    "    structural_info = []\n",
    "    base_url = \"https://rest.uniprot.org/uniprotkb/{id}.json\"\n",
    "\n",
    "    for rbp, uniprot_id in uniprot_results.items():\n",
    "        url = base_url.format(id=uniprot_id)\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            features = data.get('features', [])\n",
    "\n",
    "            binding_site_ids = []\n",
    "            motif_ids = []\n",
    "            domain_ids = []\n",
    "            motif_list = []\n",
    "            no_features = len(features)\n",
    "            \n",
    "            for feature in features:\n",
    "                feature_type = feature.get('type', 'N/A').lower()\n",
    "                \n",
    "                if \"binding site\" in feature_type or \"motif\" in feature_type:\n",
    "                    motif_list.append({\n",
    "                        'type': feature.get('type', 'N/A'),\n",
    "                        'description': feature.get('description', 'N/A'),\n",
    "                        'location': feature.get('location', 'N/A'),\n",
    "                        'evidences': feature.get('evidences', [])\n",
    "                    })\n",
    "\n",
    "                # Extraction des IDs ou evidenceCode pour chaque type : Binding site, Motif, Domain\n",
    "                if \"binding site\" in feature_type:\n",
    "                    for evidence in feature.get('evidences', []):\n",
    "                        evidence_id = evidence.get('id') or evidence.get('evidenceCode')\n",
    "                        if evidence_id is not None and evidence_id not in binding_site_ids:\n",
    "                            binding_site_ids.append(evidence_id)\n",
    "                \n",
    "                elif \"motif\" in feature_type:\n",
    "                    for evidence in feature.get('evidences', []):\n",
    "                        evidence_id = evidence.get('id') or evidence.get('evidenceCode')\n",
    "                        if evidence_id is not None and evidence_id not in motif_ids:\n",
    "                            motif_ids.append(evidence_id)\n",
    "                \n",
    "                elif \"domain\" in feature_type:\n",
    "                    for evidence in feature.get('evidences', []):\n",
    "                        evidence_id = evidence.get('id') or evidence.get('evidenceCode')\n",
    "                        if evidence_id is not None and evidence_id not in domain_ids:\n",
    "                            domain_ids.append(evidence_id)\n",
    "\n",
    "            # Compilation des informations pour chaque RBP avec les colonnes demandées\n",
    "            structural_info.append({\n",
    "                'rbp_name': rbp,\n",
    "                'uniprotID': uniprot_id,\n",
    "                'No_Features': no_features,  # Total des caractéristiques\n",
    "                'motifs': \";\".join(motif_ids),  # IDs de Motif séparés par ;\n",
    "                'binding_Sites': \";\".join(binding_site_ids),  # IDs de Binding site séparés par ;\n",
    "                'domains': \";\".join(domain_ids),  # IDs de Domain séparés par ;\n",
    "                'motif_list': motif_list \n",
    "            })\n",
    "        else:\n",
    "            structural_info.append({\n",
    "                'rbp_name': rbp,\n",
    "                'uniprotID': uniprot_id,\n",
    "                'No_Features': 0,  # 0 si échec de récupération\n",
    "                'motifs': \"\",\n",
    "                'binding_Sites': \"\",\n",
    "                'domains': \"\",\n",
    "                'motif_list': []  # Vide en cas d'erreur\n",
    "            })\n",
    "\n",
    "    return structural_info\n",
    "\n",
    "structural_results = fetch_structural_info(uniprot_results)\n",
    "\n",
    "df_structural = pd.DataFrame([{\n",
    "    'rbp_name': info['rbp_name'],\n",
    "    'uniprotID': info['uniprotID'],\n",
    "    'No_Features': info['No_Features'],\n",
    "    'motifs': info['motifs'],\n",
    "    'binding_Sites': info['binding_Sites'],\n",
    "    'domains': info['domains']\n",
    "} for info in structural_results])\n",
    "\n",
    "# Enregistrement du DataFrame dans le fichier CSV\n",
    "df_structural.to_csv('RBP_infos.csv', index=False)\n",
    "print(\"Informations structurelles sauvegardées dans le fichier 'RBP_infos.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
